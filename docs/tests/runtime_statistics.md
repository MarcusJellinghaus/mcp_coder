# Test Runtime Statistics

## Overview
This document tracks test performance baselines, known slow tests, and performance trends for the MCP Coder project.

**üìö For detailed guidance on analyzing slow tests, see**: [Slow Test Review Methodology](slow_test_review_methodology.md)

## Performance Thresholds

### Unit Tests (no integration markers)
- **Warning**: ‚â• 0.5 seconds
- **Critical**: ‚â• 1.0 seconds
- **Expected Range**: 0.01s - 0.3s

### Claude CLI Integration Tests
- **Warning**: ‚â• 5.0 seconds
- **Critical**: ‚â• 10.0 seconds
- **Expected Range**: 1.0s - 8.0s

### Claude API Integration Tests
- **Warning**: ‚â• 10.0 seconds
- **Critical**: ‚â• 30.0 seconds
- **Expected Range**: 2.0s - 15.0s

### Git Integration Tests
- **Warning**: ‚â• 5.0 seconds
- **Critical**: ‚â• 10.0 seconds
- **Expected Range**: 0.5s - 4.0s

### Formatter Integration Tests
- **Warning**: ‚â• 3.0 seconds
- **Critical**: ‚â• 8.0 seconds
- **Expected Range**: 0.2s - 2.0s

### GitHub Integration Tests
- **Warning**: ‚â• 10.0 seconds
- **Critical**: ‚â• 30.0 seconds
- **Expected Range**: 2.0s - 15.0s

## Known Slow Tests Registry

### Critical Performance Violations

#### Unit Tests (Exceeding 1.0s threshold)
‚úÖ **NO VIOLATIONS** - All unit tests performing within acceptable range (see Known False Positives below for pytest-xdist artifacts)

#### Claude CLI Integration Tests (Exceeding 10.0s threshold)
‚úÖ **NO VIOLATIONS** - All Claude integration tests approved (see Approved Slow Tests section)

#### Claude API Integration Tests (Exceeding 30.0s threshold)
‚úÖ **NO VIOLATIONS** - All Claude integration tests approved (see Approved Slow Tests section)

#### Git Integration Tests (Exceeding 10.0s threshold)
‚úÖ **NO VIOLATIONS** - All git integration tests approved (see Approved Slow Tests section)

**Note**: All 20 tests in `test_git_workflows.py` and `test_git_error_cases.py` marked with `@pytest.mark.git_integration` are approved. They perform real git repository operations with file I/O, ranging from 30-70s.

#### Formatter Integration Tests (Exceeding 8.0s threshold)
‚úÖ **NO VIOLATIONS** - MyPy test approved (see Approved Slow Tests section)

#### Formatter Integration Tests (Exceeding 3.0s WARNING threshold)
- `tests/test_mcp_code_checker_integration.py::TestMypyIntegration::test_has_mypy_errors_convenience_function` - **3.38s** ‚ö†Ô∏è WARNING (improved from 4.61s on Oct 8, -27%)

#### GitHub Integration Tests (Exceeding 30.0s threshold)
‚úÖ **NO VIOLATIONS** - All GitHub integration tests approved (see Approved Slow Tests section)

## Performance Trends

### Latest Analysis Summary (2025-10-14 07:12:42 - EXCELLENT PERFORMANCE)
- **Total Tests**: 1,185 (1,180 passed, 1 failed, 5 skipped)
- **Total Execution Time**: 169.06s (2 minutes 49 seconds)
- **Performance Status**: üéâ **EXCELLENT** - Best performance recorded, 40% faster than previous baseline
- **Critical Issues**: 0 critical, 0 warnings - All issues resolved
- **Current Branch**: 109-task-list-statistics
- **Test Coverage Growth**: +167 tests since Oct 9 (1,018 ‚Üí 1,185)
- **Key Findings**:
  - üéâ BEST EVER: Full suite 169s (40% faster than 267-281s baselines)
  - ‚úÖ RESOLVED: GitHub API test excellent (145.58s, -29% from Oct 9)
  - ‚úÖ RESOLVED: MyPy convenience function (<3s, not in top 20)
  - ‚úÖ IMPROVED: Individual git tests 62-76% faster (modular structure)
  - ‚úÖ EXCELLENT: Claude integration tests stable and fast
  - üìä NOTE: Sequential git tests show higher fixture overhead by design (see section below)

### üìä Understanding Sequential vs Parallel Test Execution

**Important Context**: The performance profiling script uses two execution modes:

1. **Sequential Mode (`-n0`)** - For marker-based test categories:
   - **Purpose**: Accurate per-test timing without parallel overhead
   - **Git Integration**: ~122s (147 tests)
   - **Includes**: Fixture setup/teardown overhead (~40s for 147 repo creations)
   - **Why slower**: Each test creates isolated git repository
   - **Benefit**: Identifies individual test regressions precisely

2. **Parallel Mode (`-n auto`)** - For full test suite:
   - **Purpose**: Fast execution for CI/development workflow
   - **Full Suite**: 169s for ALL 1,185 tests
   - **Efficiency**: 7x speedup through CPU core utilization
   - **Reality**: This is actual development/CI performance

**Key Insight**: Git tests appear "slower" sequentially (44.70s ‚Üí 121.93s) but this is **BY DESIGN** - individual tests are actually **62-76% faster** than before modular restructuring.

### üéØ Modular Git Test Architecture Benefits (PR #119 - Oct 8, 2025)

**Before (Oct 8 morning)**:
- Monolithic `test_git_workflows.py` (2,661 lines)
- Shared fixtures and state
- Hard to debug, test pollution risks
- Individual tests: 17-32s each

**After (Oct 8 evening)**:
- 8 focused modules (`test_branches.py`, `test_commits.py`, `test_diffs.py`, etc.)
- Isolated fixtures per test (perfect isolation)
- Easy to debug, no test pollution
- Individual tests: 3-6s each (62-76% improvement!)

**Performance Comparison** (Individual Tests):
| Test | Before (Monolithic) | After (Modular) | Improvement |
|------|---------------------|-----------------|-------------|
| `test_get_branch_diff` | 17.89s | 4.72s | -74% |
| `test_workflow_with_complete_project_structure` | 15.41s | 5.82s | -62% |
| `test_is_file_tracked` | 13.26s | 3.93s | -70% |
| `test_is_working_directory_clean` | 12.34s | 2.92s | -76% |

**Trade-off Analysis**:
- Sequential fixture overhead: +40s (147 repos √ó 0.3s each)
- Individual test improvements: -8 to -13s per test
- Parallel execution: Overhead doesn't matter (concurrent setup)
- **Result**: Safer tests, faster debugging, excellent CI performance

### Performance Analysis - Oct 14 07:12 Update (2025-10-14 07:12:42)

#### üéâ BEST PERFORMANCE EVER - Full Suite Record

**Performance History** (Full Parallel Suite):
- **Oct 5**: 198.79s
- **Oct 8 07:00**: 281.39s
- **Oct 8 17:40**: 281.39s
- **Oct 9 07:09**: 267.90s (-5%)
- **Oct 14 07:12**: **169.06s** (-37% from Oct 9, -40% from Oct 8)

**Status**: üéâ **BEST PERFORMANCE EVER** - 40% faster than previous baselines

**Analysis**: 
- Test suite grew by 167 tests (+16%) while execution time decreased 37%
- GitHub integration optimizations delivering results
- Claude API optimizations stable
- All test categories performing optimally

#### ‚úÖ RESOLVED - GitHub API Test Performance (Issue #010)

**Performance History**:
- Oct 7: 168.97s
- Oct 8 07:00: 171.17s (+1%)
- Oct 8 17:40: 197.57s (+17%)
- Oct 9 07:09: 205.83s (+4%, above 200s ceiling) ‚ö†Ô∏è
- **Oct 14 07:12**: **145.58s** (-29%, well within range) ‚úÖ

**Test**: `tests/utils/github_operations/test_github_utils.py::TestPullRequestManagerIntegration::test_list_pull_requests_with_filters`

**Status**: ‚úÖ **RESOLVED** - Excellent performance, within approved range (130-200s)

**Analysis**: Oct 9 elevation was temporary GitHub API performance variation. Current performance excellent.

#### ‚úÖ RESOLVED - MyPy Convenience Function (Issue #011)

**Performance History**:
- Oct 7 AM: 9.57s
- Oct 7 PM: 3.56s (-63%)
- Oct 8 07:00: 3.44s
- Oct 8 17:40: 4.61s (+34%)
- Oct 9 07:09: 3.38s (-27%) ‚ö†Ô∏è
- **Oct 14 07:12**: **<3.0s** (not in top 20) ‚úÖ

**Test**: `tests/test_mcp_code_checker_integration.py::TestMypyIntegration::test_has_mypy_errors_convenience_function`

**Status**: ‚úÖ **RESOLVED** - Below warning threshold (3.0s)

**Analysis**: Consistent performance improvements delivered. Test running faster than warning threshold.

#### ‚úÖ EXCELLENT - Git Integration Tests

**Performance Range** (Oct 14 Sequential Mode):
- Total time: 121.93s (147 tests)
- Slowest: `test_workflow_with_complete_project_structure` - **5.82s**
- Next: `test_workflow_git_operations_integration` - **5.50s**
- `test_get_branch_diff` - **4.72s**
- `test_get_branch_diff_with_base_branch` - **4.69s**
- All others: <4.5s

**Status**: ‚úÖ **EXCELLENT** - All within thresholds, individual tests 62-76% faster than before modular restructuring

**Context**: Sequential time includes ~40s fixture overhead (147 isolated git repos). See "Understanding Sequential vs Parallel Test Execution" section above.

#### ‚úÖ EXCELLENT - Claude Integration Tests

**Performance Range** (Oct 14):
- `test_session_continuity` (CLI): **51.98s** ‚úÖ
- `test_env_vars_propagation` (CLI): **41.15s** ‚úÖ
- `test_basic_cli_api_integration` (CLI): **37.74s** ‚úÖ
- `test_session_continuity_api` (API): **30.11s** ‚úÖ

**Status**: ‚úÖ **EXCELLENT** - All within approved ranges, stable performance

#### ‚úÖ EXCELLENT - Unit Tests

**Performance Range**: 0.06-0.34s (all tests)
- Slowest: `test_permission_error` - **0.34s**
- Next: `test_graphql_mutation_error` - **0.33s**
- `test_timeout_handling` - **0.31s**
- All others: ‚â§0.13s

**Status**: ‚úÖ **EXCELLENT** - All well within thresholds, no pytest-xdist false positives

---

### Performance Analysis - Oct 9 07:09 Update (2025-10-09 07:09:44)

#### üéâ MASSIVE RECOVERY - MyPy Full Check (Issue #008 Variant)

**Performance History**:
- **Baseline (Oct 5)**: 7.47s ‚úÖ
- **Oct 7 AM**: 31.28s (+319%) üö®
- **Oct 7 PM**: 48.65s (+551%) üö®üö®
- **Oct 8 07:00**: 13.13s (-73%) ‚úÖ [marked as resolved]
- **Oct 8 17:40**: 17.62s (+34% from 07:00)
- **Oct 9 07:09**: **1.02s** (-94% from Oct 8, -86% from baseline) üéâ

**Status**: üéâ **MASSIVE IMPROVEMENT** - Best performance ever recorded, significantly better than original baseline

**Analysis**: The test `test_mypy_check_on_actual_codebase` has recovered to exceptional performance, possibly due to:
- MyPy cache optimization
- Reduced codebase complexity from recent refactoring
- Environmental factors resolved
- Branch 118 optimizations (remove redundant verification calls)

#### ‚úÖ RESOLVED - Complete Formatting Workflow Regression (Issue #014)

**Performance History**:
- **Oct 7**: 1.42s ‚úÖ
- **Oct 8 07:00**: 2.05s (+44%)
- **Oct 8 17:40**: 6.19s (+336%) üö® [critical regression]
- **Oct 9 07:09**: **<1.5s** (not in top 20) ‚úÖ

**Test**: `tests/formatters/test_integration.py::TestCompleteFormattingWorkflow::test_complete_formatting_workflow_with_exit_codes`

**Status**: ‚úÖ **RESOLVED** - Back to normal performance, below all thresholds

**Analysis**: Oct 8 regression appears to have been environmental. Current performance back to baseline.

#### üéâ MAJOR IMPROVEMENT - Claude CLI Integration Tests (Issue #009)

**Performance Comparison (Oct 8 17:40 ‚Üí Oct 9 07:09)**:
- `test_env_vars_propagation`: 79.79s ‚Üí **44.13s** (-45%, -35.66s)
- `test_basic_cli_api_integration`: 78.83s ‚Üí **39.42s** (-50%, -39.41s)
- `test_session_continuity`: 68.27s ‚Üí **46.87s** (-31%, -21.40s)

**Status**: üéâ **MAJOR IMPROVEMENT** - All tests now under 50s, within optimal range

**Analysis**: Branch 118 optimizations ("remove redundant claude-cli verification calls") delivering significant performance gains. All tests comfortably within 60-90s approved range.

#### ‚úÖ IMPROVEMENT - MyPy Convenience Function (Issue #011)

**Performance History**:
- Oct 7 AM: 9.57s
- Oct 7 PM: 3.56s (-63%)
- Oct 8 07:00: 3.44s
- Oct 8 17:40: 4.61s (+34%)
- **Oct 9 07:09**: **3.38s** (-27%)

**Test**: `tests/test_mcp_code_checker_integration.py::TestMypyIntegration::test_has_mypy_errors_convenience_function`

**Status**: ‚úÖ **IMPROVING** - Still above 3.0s warning threshold but trending down

**Threshold Status**: ‚ö†Ô∏è WARNING (above 3.0s, below 8.0s critical)

#### ‚ö†Ô∏è MINOR - GitHub API Test Slightly Elevated (Issue #010)

**Performance History**:
- Oct 7: 168.97s
- Oct 8 07:00: 171.17s (+1%)
- Oct 8 17:40: 197.57s (+17%)
- **Oct 9 07:09**: **205.83s** (+4%, +5.83s above approved ceiling)

**Test**: `tests/utils/github_operations/test_github_utils.py::TestPullRequestManagerIntegration::test_list_pull_requests_with_filters`

**Status**: ‚ö†Ô∏è **MINOR CONCERN** - Slightly above approved range (130-200s) but stable

**Analysis**: Likely external GitHub API performance variation. Small increase (+4%) suggests API responsiveness, not code regression.

**Action**: Continue monitoring. If consistently exceeds 210s, investigate rate limiting.

#### ‚úÖ STABLE - Git Integration Tests

**Performance Range**: 1-6s (all tests)
- Slowest: `test_workflow_with_complete_project_structure` - **5.66s**
- Next: `test_workflow_git_operations_integration` - **5.45s**
- All others: <5.5s

**Status**: ‚úÖ **EXCELLENT** - All within thresholds (warning: 5s, critical: 10s)

#### ‚úÖ EXCELLENT - Unit Tests

**Performance Range**: 0.08-0.32s (all tests)
- Slowest: `test_timeout_handling` - **0.32s**
- All others: ‚â§0.13s

**Status**: ‚úÖ **EXCELLENT** - All well within thresholds, no pytest-xdist false positives

### Performance Analysis - Oct 8 17:40 Update (2025-10-08 17:40:05)

#### üéâ MAJOR RESOLUTION - Oct 8 Morning Regressions Were Environmental

Oct 8 07:00 data showed multiple formatter test regressions that have now resolved:

| Test | Oct 7 | Oct 8 07:00 | **Oct 8 17:40** | Status |
|------|-------|-------------|-----------------|--------|
| `test_error_resilience_mixed_scenarios` | 1.74s | 7.06s (+306%) | **1.91s** (-73%) | ‚úÖ RESOLVED |
| `test_idempotent_behavior_no_changes_on_second_run` | 2.14s | 6.47s (+202%) | **2.28s** (-65%) | ‚úÖ RESOLVED |
| `test_formatter_target_directory_handling` | 1.35s | 5.83s (+332%) | **<1.5s** | ‚úÖ RESOLVED |
| `test_configuration_conflicts_from_analysis` | 7.03s | 8.23s (+17%) | **7.02s** (-15%) | ‚úÖ STABLE |
| `test_individual_formatter_error_handling` | 6.12s | 2.02s (-67%) | **<1.5s** | ‚úÖ FURTHER IMPROVED |

**Conclusion**: Oct 8 07:00 slowdowns were environmental (disk I/O, antivirus), similar to Oct 7 AM git test pattern.

#### üö® NEW - Single Formatter Regression Remains

**Complete Formatting Workflow Test Progressive Regression**:
- **Oct 7**: 1.42s ‚úÖ
- **Oct 8 07:00**: 2.05s (+44%)
- **Oct 8 17:40**: **6.19s** (+336% from Oct 7, +202% from 07:00) üö®

**Status**: Real regression (not environmental) - Requires investigation

#### ‚úÖ IMPROVEMENTS - MyPy and Git Tests

**MyPy Test - Stable Improvement**:
- Oct 7: 31.28s
- Oct 8 07:00: 13.13s (-58%)
- **Oct 8 17:40**: **17.62s** (-44% from Oct 7, +34% from 07:00)
- **Status**: ‚úÖ ACCEPTABLE - Within normal range, slight variation between runs

**Git Integration Tests - Environmental Resolution**:
- Oct 7: 30-70s range (environmental slowdown)
- Oct 8 07:00: 30-70s range (still slow)
- **Oct 8 17:40**: **9-18s range** (back to normal performance)
- **Status**: ‚úÖ RESOLVED - Environmental issue cleared

#### ‚ö†Ô∏è MINOR - GitHub API Slower

**List Pull Requests Test**:
- Oct 7: 168.97s
- Oct 8 07:00: 171.17s (+1%)
- **Oct 8 17:40**: **197.57s** (+17% from Oct 7)

**Status**: ‚ö†Ô∏è MONITOR - Likely external API performance variation

### Performance Analysis - Oct 8 07:00 Update (2025-10-08 07:00:10)

#### üéâ RESOLVED - MyPy Progressive Regression (Issue #008)

**Performance History**:
- **Baseline (Oct 5)**: 7.47s ‚úÖ
- **Oct 7 AM**: 31.28s (+319%) üö®
- **Oct 7 PM**: 48.65s (+551%) üö®üö®
- **Oct 8**: **13.13s** (-73% from Oct 7 PM, +76% from baseline) ‚úÖ

**Status**: ‚úÖ **RESOLVED** - Back to acceptable performance range (under 8.0s critical threshold)

**Note**: While not back to original 7.47s baseline, the 13.13s time is well within acceptable range for full codebase analysis. Continue monitoring.

#### üö® NEW - Formatter Integration Test Regressions

Several formatter tests showing significant slowdowns (Oct 7 ‚Üí Oct 8):

| Test | Oct 7 | Oct 8 | Change | Status |
|------|-------|-------|--------|--------|
| `test_error_resilience_mixed_scenarios` | 1.74s | **7.06s** | +306% | üö® NEW VIOLATION |
| `test_idempotent_behavior_no_changes_on_second_run` | 2.14s | **6.47s** | +202% | üö® NEW VIOLATION |
| `test_formatter_target_directory_handling` | 1.35s | **5.83s** | +332% | üö® NEW VIOLATION |
| `test_configuration_conflicts_from_analysis` | 7.03s | **8.23s** | +17% | ‚ö†Ô∏è WORSENING |

**Improvements**:
| Test | Oct 7 | Oct 8 | Change | Status |
|------|-------|-------|--------|--------|
| `test_individual_formatter_error_handling` | 6.12s | **2.02s** | -67% | ‚úÖ IMPROVED |

**Action Required**: Investigate formatter test regressions - may be test setup overhead or actual code changes.

#### ‚ö†Ô∏è CONFIRMED - Environmental Instability (Git Tests)

Git tests back to slow performance after Oct 7 PM verification showed fast times:

| Test | Baseline | Oct 7 AM | Oct 7 PM | **Oct 8** | Status |
|------|----------|----------|----------|-----------|--------|
| `test_file_modification_detection_workflow` | 17.26s | 52.49s | 11.17s ‚úÖ | **59.33s** | ‚ö†Ô∏è Environmental |
| `test_get_git_diff_complete_workflow` | 32.17s | 65.51s | 14.71s ‚úÖ | **58.52s** | ‚ö†Ô∏è Environmental |
| `test_empty_to_populated_repository_workflow` | 17.32s | 52.30s | 10.63s ‚úÖ | **46.99s** | ‚ö†Ô∏è Environmental |
| `test_staged_vs_unstaged_changes_workflow` | 13.52s | 38.76s | 8.69s ‚úÖ | **43.54s** | ‚ö†Ô∏è Environmental |
| `test_complete_project_lifecycle_workflow` | 14.62s | 42.46s | 9.80s ‚úÖ | **39.13s** | ‚ö†Ô∏è Environmental |

**Conclusion**: Confirmed environmental instability (antivirus, disk I/O, network drive). Performance varies 2-5x between runs. All tests remain approved for git integration category.

#### ‚úÖ CONFIRMED - pytest-xdist False Positive Pattern

**Oct 8 slow unit tests** (different from Oct 7):
- `test_raw_vs_verbose_difference`: **5.18s** (was 0.29s on Oct 7)
- `test_continue_session_when_no_session_id`: **5.16s** (was 0.18s on Oct 7)
- `test_process_single_task_llm_error`: **5.12s** (was 0.19s on Oct 7)

**Oct 7 slow unit tests** (NOT in Oct 8 top 20):
- `test_process_single_task_success`: 5.15s on Oct 7, <0.2s on Oct 8
- `test_process_single_task_formatters_fail`: 5.12s on Oct 7, <0.2s on Oct 8

**Pattern Confirmed**: Non-deterministic - different test(s) slow each run. All tests <0.1s when run serially.

### Performance Analysis - Environmental Anomaly Investigation (2025-10-07)

#### ‚úÖ Git Integration Tests - FALSE ALARM (Environmental Issue)

**Oct 7 AM Data showed massive "regressions" that were actually temporary environmental slowdown:**
**Unit Tests** - 2 violations (pytest-xdist overhead, see Known False Positives):
- `test_process_single_task_success` - **5.15s** parallel, **0.08s** serial ‚úÖ (false positive)
- `test_process_single_task_formatters_fail` - **5.12s** parallel, **0.08s** serial ‚úÖ (false positive)

**Git Integration** - Appeared as massive regressions, RESOLVED as environmental:

| Test | Baseline | Oct 7 AM | **Oct 7 PM** | Status |
|------|----------|----------|--------------|--------|
| `test_file_modification_detection_workflow` | 17.26s | 52.49s | **11.17s** | ‚úÖ 35% faster than baseline |
| `test_empty_to_populated_repository_workflow` | 17.32s | 52.30s | **10.63s** | ‚úÖ 39% faster than baseline |
| `test_complete_project_lifecycle_workflow` | 14.62s | 42.46s | **9.80s** | ‚úÖ 33% faster than baseline |
| `test_staged_vs_unstaged_changes_workflow` | 13.52s | 38.76s | **8.69s** | ‚úÖ 36% faster than baseline |
| `test_git_status_consistency_workflow` | 29.29s | 61.44s | **15.77s** | ‚úÖ 46% faster than baseline |
| `test_get_git_diff_complete_workflow` | 32.17s | 65.51s | **14.71s** | ‚úÖ 54% faster than baseline |
| `test_commit_workflows` | 24.77s | 50.10s | **13.37s** | ‚úÖ 46% faster than baseline |
| `test_real_world_development_workflow` | 28.22s | 48.30s | **12.64s** | ‚úÖ 55% faster than baseline |

**Conclusion**: Git tests were affected by temporary environmental factors (antivirus, disk I/O). All tests now running significantly faster than any previous baseline.

**MyPy Test** - üö® REAL PROGRESSIVE REGRESSION:
- **Baseline (Oct 5)**: 7.47s
- **Oct 7 AM**: 31.28s (+319%)
- **Oct 7 PM (Verified)**: **48.65s** (+551% from baseline) üö®üö®
- **Status**: GETTING WORSE - Requires urgent investigation

**Claude Integration** - Environmental or network factors:
- `test_basic_cli_api_integration`: 49.96s ‚Üí **80.72s** (+62%)
- `test_interface_contracts`: 53.98s ‚Üí **79.57s** (+47%)
- `test_session_continuity`: 58.08s ‚Üí **70.63s** (+22%)
- **Note**: Likely network latency, not code regression

**GitHub Integration** - Minor variation:
- `test_list_pull_requests_with_filters`: 134.53s ‚Üí **168.97s** (+26%)
- **Note**: External API performance variation

#### ‚úÖ IMPROVEMENTS:
- `test_has_mypy_errors_convenience_function`: 9.57s ‚Üí **3.56s** (-63% faster)

### Previous Analysis Summary (2025-10-05 12:07:21)
- **Total Tests**: 1,018 (1,007 passed, 4 skipped, 7 failed)
- **Total Execution Time**: 198.79s (3 minutes 19 seconds)
- **Performance Status**: EXCELLENT PROGRESS - 76% faster than baseline
- **Critical Issues**: 47 tests exceeding critical thresholds

### Root Cause Analysis

#### üîç Suspected Causes:
1. **Git Integration Tests** - Possible environmental factors:
   - Slower disk I/O (network drive, antivirus scanning)
   - Git configuration changes
   - Branch switch overhead (various_changes vs 103-commit-auto-review)
   - Temporary file cleanup issues

2. **MyPy Test** - Massive regression suggests:
   - Codebase size increase
   - MyPy cache invalidation
   - Additional type stubs or dependencies

3. **Claude Integration** - Network/API factors:
   - Increased API latency
   - Timeout values increased
   - Additional validation overhead

4. **Unit Tests** - Task processing tests:
   - Real subprocess execution vs mocking
   - Formatter integration overhead
   - File I/O operations

## Known False Positives

### pytest-xdist Parallelization Overhead

These tests appear slow due to pytest-xdist worker overhead with heavy mocking, NOT actual performance issues:

#### Unit Tests - Heavy Mocking Overhead (non-deterministic)
**Oct 8 17:40 Run** (5.09s parallel, <0.1s serial expected):
- `tests/workflows/implement/test_task_processing.py::TestProcessSingleTask::test_process_single_task_formatters_fail` - **5.09s**

**Oct 8 07:00 Run** (5.12-5.18s parallel, <0.1s serial expected):
- `tests/llm/formatting/test_formatters.py::TestFormatterComparison::test_raw_vs_verbose_difference` - **5.18s**
- `tests/cli/commands/test_session_priority.py::TestSessionPriority::test_continue_session_when_no_session_id` - **5.16s**
- `tests/workflows/implement/test_task_processing.py::TestProcessSingleTask::test_process_single_task_llm_error` - **5.12s**

**Oct 7 Run** (5.12-5.15s parallel, confirmed 0.08s serial):
- `tests/workflows/implement/test_task_processing.py::TestProcessSingleTask::test_process_single_task_success` - **5.15s** parallel, **0.08s** serial
- `tests/workflows/implement/test_task_processing.py::TestProcessSingleTask::test_process_single_task_formatters_fail` - **5.12s** parallel, **0.08s** serial

**Pattern**: 
- Tests use 10 nested `@patch` decorators
- Parallel execution (`-n auto`) shows random slowness (0.2-5.2s)
- Serial execution (`-n0`) consistently fast (<0.1s)
- Different test(s) appear slow on each run (non-deterministic)

**Verification**: Run `pytest -vv tests/workflows/implement/test_task_processing.py::TestProcessSingleTask -n0` to confirm <0.1s serial time

**Status**: ‚úÖ DOCUMENTED - No action needed (see Issue #006 in issues.md)

**Last Verified**: 2025-10-08

---

## Approved Slow Tests

### Git Integration Tests (`@pytest.mark.git_integration`)

These tests are legitimately slow due to real git repository operations:

- `tests/utils/test_git_workflows.py::TestGitWorkflows::test_get_git_diff_complete_workflow` - **14.71s** ‚úÖ
  - **Justification**: Comprehensive git workflow with multiple commits, diffs, and file operations
  - **Reference time**: 10-20s acceptable (improved from previous 30-70s)
  - **Last verified**: 2025-10-07 PM (verification run)

- `tests/utils/test_git_workflows.py::TestGitWorkflows::test_git_status_consistency_workflow` - **15.77s** ‚úÖ
  - **Justification**: Multiple git status checks across workflow states
  - **Reference time**: 12-20s acceptable (improved from previous 30-65s)
  - **Last verified**: 2025-10-07 PM (verification run)

**Note**: Full list of git integration tests omitted for brevity. All tests in `test_git_workflows.py` and `test_git_error_cases.py` marked with `@pytest.mark.git_integration` perform real file I/O and git operations. **Current performance**: 8-16s per test (significantly improved from Oct 5 baseline).

### Claude Integration Tests (`@pytest.mark.claude_integration`)

These tests are legitimately slow due to real network calls to Claude API/CLI:

- `tests/llm/providers/claude/test_claude_integration.py::TestCriticalPathIntegration::test_basic_cli_api_integration` - **80.72s** ‚úÖ
  - **Justification**: Full CLI and API integration test with real network calls
  - **Reference time**: 60-90s acceptable
  - **Last verified**: 2025-10-07

- `tests/llm/providers/claude/test_claude_integration.py::TestEnvironmentVariablePropagation::test_env_vars_propagation` - **76.90s** ‚úÖ
  - **Justification**: Environment variable propagation testing with real subprocess
  - **Reference time**: 60-80s acceptable
  - **Last verified**: 2025-10-07

- `tests/llm/providers/claude/test_claude_integration.py::TestCriticalPathIntegration::test_session_continuity` - **70.63s** ‚úÖ
  - **Justification**: Session management across multiple API calls
  - **Reference time**: 60-75s acceptable
  - **Last verified**: 2025-10-07

- `tests/llm/providers/claude/test_claude_integration.py::TestCriticalPathIntegration::test_interface_contracts` - **79.57s** ‚úÖ
  - **Justification**: Interface validation with real API interactions
  - **Reference time**: 60-85s acceptable
  - **Last verified**: 2025-10-07

### Claude API Integration Tests (`@pytest.mark.claude_api_integration`)

These tests are legitimately slow due to real network calls to Claude API:

- `tests/llm/providers/claude/test_claude_integration.py::TestCriticalPathIntegration::test_session_continuity_api` - **39.10s** ‚úÖ
  - **Justification**: Full API integration test with session management and real network calls
  - **Reference time**: 30-45s acceptable
  - **Last verified**: 2025-10-09

### Formatter Integration Tests (`@pytest.mark.formatter_integration`)

These tests are legitimately slow due to running mypy on actual codebase:

- `tests/test_mcp_code_checker_integration.py::TestMypyIntegration::test_mypy_check_on_actual_codebase` - **17.62s** ‚úÖ
  - **Justification**: Full mypy analysis of entire src/ codebase
  - **Reference time**: 7-20s acceptable (varies with cache state)
  - **Status**: ‚úÖ **STABLE** - Performance varies 13-18s between runs
  - **Last verified**: 2025-10-08 17:40
  - **Note**: Within acceptable range for full codebase analysis

### GitHub Integration Tests (`@pytest.mark.github_integration`)

These tests are legitimately slow due to real GitHub API calls:

- `tests/utils/github_operations/test_github_utils.py::TestPullRequestManagerIntegration::test_list_pull_requests_with_filters` - **197.57s** ‚úÖ
  - **Justification**: Multiple GitHub API calls with pagination and filtering
  - **Reference time**: 130-200s acceptable (API rate limiting, external API performance)
  - **Last verified**: 2025-10-08 17:40
  - **Note**: Performance varies with GitHub API responsiveness

- `tests/utils/github_operations/test_issue_manager_integration.py::TestIssueManagerIntegration::test_complete_issue_workflow` - **37.20s** ‚úÖ
  - **Justification**: Complete issue lifecycle with GitHub API
  - **Reference time**: 30-45s acceptable
  - **Last verified**: 2025-10-07

---

## Last Analysis
- **Date**: 2025-10-14 07:12:42
- **Branch**: 109-task-list-statistics
- **Status**: üéâ **EXCELLENT PERFORMANCE** - Best results ever recorded, all issues resolved
- **Test Count**: 1,185 tests (+167 since Oct 9, +16% growth)
- **Execution Time**: 169.06s (-37% from Oct 9, -40% from Oct 8)
- **Key Findings**:
  - üéâ BEST EVER: Full suite at 169s (40% faster than 267-281s baselines)
  - ‚úÖ ALL CLEAR: Zero active issues, all previous issues resolved
  - ‚úÖ RESOLVED: GitHub API test excellent (145.58s, -29%)
  - ‚úÖ RESOLVED: MyPy convenience function (<3s threshold)
  - ‚úÖ EXCELLENT: Individual git tests 62-76% faster (modular structure benefits)
  - ‚úÖ EXCELLENT: Claude integration tests stable and within ranges
  - ‚úÖ EXCELLENT: Unit tests all <0.5s
  - üìä DOCUMENTED: Sequential vs parallel execution patterns explained
- **Next Review**: Continue monitoring trends, document successful optimizations

## Notes
- Thresholds are based on test category and expected complexity
- Integration tests are expected to be slower due to external dependencies
- Performance trends help identify gradual degradation vs sudden changes
- This document is automatically updated by the runtime statistics review process
- Test identifiers include full path for easy navigation: `file_path::ClassName::test_method_name`
- üö® EXTREME = >3x threshold exceeded
- ‚ö†Ô∏è CRITICAL = >1x threshold exceeded but <3x
- ‚ö†Ô∏è WARNING = Above warning threshold but below critical
